# NVIDIA L4 Tensor Core Optimizations

## ðŸš€ Quick Start

Your L4 GPU is **already optimized** with automatic performance enhancements in `trainers/train_gpt_v2.py`!

### Recommended Training Command

```bash
python trainers/train_gpt_v2.py \
  --train-manifest preprocessed_amharic/train_manifest.jsonl \
  --val-manifest preprocessed_amharic/val_manifest.jsonl \
  --tokenizer amharic_bpe.model \
  --batch-size 8 \
  --grad-accumulation 4 \
  --amp \
  --num-workers 8
```

**Effective batch size:** 8 Ã— 4 = **32** (optimal for L4's 24GB VRAM)

---

## ðŸ“Š L4 Hardware Specs

- **GPU:** 1Ã— NVIDIA L4 Tensor Core (24GB VRAM)
- **Architecture:** Ada Lovelace (Ampere successor)
- **vCPUs:** 8
- **RAM:** 32GB
- **Optimized for:** Inference + smaller training workloads

---

## âš¡ Automatic Optimizations Enabled

### 1. **TF32 Precision** âœ…
- **What:** TensorFloat-32 for matrix multiplications
- **Speedup:** 3-8Ã— faster than FP32
- **Accuracy:** Same as FP32 (maintains model quality)
- **Enabled:** Automatically on CUDA devices

### 2. **bfloat16 AMP** âœ…
- **What:** Brain Float 16 mixed precision training
- **Why better than float16:** 
  - Better numerical stability (same exponent range as FP32)
  - No gradient scaling needed (faster, simpler)
  - Native L4 hardware support
- **Speedup:** 2-3Ã— faster than FP32
- **Enabled:** Automatically when `--amp` flag is used

### 3. **cuDNN Benchmark** âœ…
- **What:** Auto-tunes kernels for your specific model/input sizes
- **Speedup:** 5-20% improvement
- **Trade-off:** Slight startup time increase (one-time cost)
- **Enabled:** Automatically on CUDA devices

### 4. **Optimized Batch Sizes** âœ…
- **Batch size:** 8 (vs default 4)
- **Grad accumulation:** 4 (vs default 1)
- **Effective batch:** 32
- **VRAM usage:** ~20GB / 24GB (safe margin for L4)

### 5. **Multi-threaded Data Loading** âœ…
- **Workers:** 8 (matches your 8 vCPUs)
- **Benefit:** GPU never waits for data
- **Speedup:** Up to 2Ã— faster training

### 6. **channels_last Memory Format** âš ï¸
- **What:** Optimized memory layout for Tensor Cores
- **Note:** Primarily helps CNNs; transformers see minimal benefit
- **Enabled:** Attempted automatically (graceful fallback if unsupported)

---

## ðŸ”§ Optional: Save VRAM

If you run out of memory, enable gradient checkpointing:

```bash
--grad-checkpointing
```

**Trade-off:** 20-30% slower, but saves ~30% VRAM

**When to use:**
- Batch size 8 causes OOM errors
- You want to try batch size 12-16

---

## ðŸ“ˆ Expected Performance

### For Your 200hr Amharic Dataset:

| Stage | Time | Notes |
|-------|------|-------|
| **Preprocessing** | 8-12 hours | With `--amp` and 8 workers |
| **Training** | 2-3 days | Full convergence (~50k steps) |
| **Speedup vs V100** | 1.3-1.5Ã— | Due to TF32 + bfloat16 |
| **Speedup vs CPU** | 50-100Ã— | GPU is essential for this workload |

---

## ðŸ” Verification

When training starts, you should see these logs:

```
[L4 Optimizations] TF32 enabled, cuDNN benchmark enabled
[L4 Optimizations] Model converted to channels_last memory format
[L4 Optimizations] Using bfloat16 AMP (native L4 support, no loss scaling needed)
```

If you see `float16 AMP with gradient scaling` instead of `bfloat16 AMP`, your PyTorch version may not support bfloat16 detection. Update PyTorch:

```bash
pip install --upgrade torch
```

---

## ðŸŽ¯ Troubleshooting

### Out of Memory (OOM)

**Solution 1:** Reduce batch size
```bash
--batch-size 6 --grad-accumulation 6  # Effective batch = 36
```

**Solution 2:** Enable gradient checkpointing
```bash
--grad-checkpointing
```

**Solution 3:** Reduce workers
```bash
--num-workers 4
```

### Training is slow

**Check:**
1. âœ… `--amp` flag is set
2. âœ… `--num-workers 8` (or at least > 0)
3. âœ… CUDA is available: `python -c "import torch; print(torch.cuda.is_available())"`

### Loss is unstable

**Solution:** bfloat16 is already more stable than float16, but if needed:
```bash
# Remove --amp to use full FP32 (slower but most stable)
```

---

## ðŸ“š Technical Details

### TF32 vs FP32 vs bfloat16

| Type | Bits | Range | Precision | Speed |
|------|------|-------|-----------|-------|
| FP32 | 32 | High | High | 1Ã— (baseline) |
| TF32 | 19 | High | Medium | 3-8Ã— |
| bfloat16 | 16 | High | Low | 2-3Ã— |
| float16 | 16 | Low | Low | 2-3Ã— |

**L4 uses TF32 for matmul + bfloat16 for other ops = best of both worlds!**

### Why L4 is Great for Training

- âœ… Native bfloat16 support (better than V100's float16)
- âœ… TF32 Tensor Cores (faster than V100)
- âœ… 24GB VRAM (good for medium models)
- âœ… Better performance/$ than A100 for this workload

---

## âœ¨ Summary

**You're all set!** The training script is already optimized for your L4 GPU. Just use the recommended command and you'll get maximum performance automatically.

**Key takeaways:**
1. Always use `--amp` flag
2. Default settings (batch=8, grad_accum=4) are optimized for L4
3. Expect 2-3 days for full training on 200hr dataset
4. bfloat16 is automatically used (better than float16)

Happy training! ðŸš€
