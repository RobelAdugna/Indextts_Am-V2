# Resume Training - Quick Reference Card

## ‚úÖ Resume Training Works Perfectly

The code in `trainers/train_gpt_v2.py` has **complete, production-ready resume functionality**.

## üöÄ Quick Start Commands

### Option 1: Smart Wrapper (Easiest)
```bash
python smart_train_amharic.py
```
**Auto-resumes if checkpoint exists, starts fresh otherwise.**

### Option 2: Manual Scripts
```bash
# Fresh start:
bash scripts/start_fresh_amharic_training.sh

# Resume:
bash scripts/resume_amharic_training.sh
```

### Option 3: Direct Python (Full Control)
```bash
# Start fresh:
python trainers/train_gpt_v2.py \
  --train-manifest preprocessed_amharic/train_pairs.jsonl \
  --val-manifest preprocessed_amharic/val_pairs.jsonl \
  --tokenizer tokenizers/amharic_extended_bpe.model \
  --output-dir trained_ckpts_fixed \
  --learning-rate 5e-6 \
  --text-loss-weight 0.4 \
  --mel-loss-weight 0.6 \
  --warmup-steps 2000 \
  --amp

# Resume:
# Add --resume auto to above command
python trainers/train_gpt_v2.py --resume auto [... same args ...]
```

## üìã What Resume Does

### Saves in Checkpoint:
- ‚úÖ Model weights (all layers)
- ‚úÖ Optimizer state (Adam momentum)
- ‚úÖ LR scheduler
- ‚úÖ Gradient scaler (AMP)
- ‚úÖ Training step counter
- ‚úÖ Epoch/batch position

### Restores on Resume:
All of the above + re-registers gradient hooks automatically!

## üîß Extended Vocab Fix Compatibility

**Gradient hooks re-register EVERY time training starts:**

```python
if vocab_size > 12000:
    # These hooks apply automatically on resume:
    model.text_embedding.weight.register_hook(freeze_base_tokens_hook)
    model.text_head.weight.register_hook(freeze_base_tokens_hook)
    # Base tokens (0-11999): Frozen ‚úÖ
    # Amharic tokens (12000-23999): Trainable ‚úÖ
```

## ‚ö†Ô∏è Your Specific Case

**DON'T resume from old checkpoint (step 38k):**
- Reason: Corrupted optimizer state from broken training
- Solution: Start fresh in new directory
- Why: Faster to good results (25k steps vs 60k+ steps)

## üìä Expected Behavior

### Fresh Training (Recommended)
```
Step 0:    text_loss=6.5, mel_loss=6.0  (random init)
Step 1k:   text_loss=3.8, mel_loss=4.2  (learning!)
Step 5k:   text_loss=2.7, mel_loss=3.2  (improving!)
Step 20k:  text_loss=1.9, mel_loss=2.3  (intelligible)
Step 50k:  text_loss=1.6, mel_loss=2.0  (production)
```

### Resume (After Fix Applied)
```
[Loading checkpoint...]
‚úÖ Successfully resumed from step 20000

[Extended Vocab Fix] Detected extended vocabulary
[Extended Vocab Fix] Gradient hooks registered

Step 20001: text_loss=1.9, mel_loss=2.3  (continues!)
```

## üõ†Ô∏è Troubleshooting

### "Checkpoint missing required keys"
**Fix:** Use previous checkpoint or start fresh

### "OOM after resume"
**Fix:** Reduce `--batch-size 4 --grad-accumulation 8`

### Loss spike after resume
**Normal:** Small spike (<0.5) is expected, recovers quickly

### No "Extended Vocab Fix" messages
**Check:** Tokenizer size with `python verify_amharic_training.py`

## üìÇ Files Created

**Documentation:**
- `COMPLETE_RESUME_TRAINING_GUIDE.md` - Complete guide
- `RESUME_QUICK_REFERENCE.md` - This file
- `RESUME_TRAINING_WITH_FIX.md` - Technical details
- `RESUME_QUICK_ANSWER.md` - Concise FAQ

**Executable Scripts:**
- `smart_train_amharic.py` - Smart wrapper
- `scripts/start_fresh_amharic_training.sh` - Fresh start
- `scripts/resume_amharic_training.sh` - Resume

## ‚úÖ Verification Checklist

Before training:
- [ ] `python check_amharic_data.py` - Data quality OK?
- [ ] `python verify_amharic_training.py` - Tokenizer = 24k?
- [ ] Prompt-target pairs generated?

During training (first 5k steps):
- [ ] See "[Extended Vocab Fix]" messages? 
- [ ] text_loss dropping? (should be <3.0 by step 5k)
- [ ] No OOM errors?

After interruption:
- [ ] Just add `--resume auto` to same command
- [ ] Training continues from saved step?

## üéØ Summary

| Feature | Status | Notes |
|---------|--------|-------|
| Resume functionality | ‚úÖ Production-ready | Built into train_gpt_v2.py |
| Extended vocab compatible | ‚úÖ Yes | Hooks re-register automatically |
| Old checkpoint (38k) | ‚ùå Don't use | Corrupted optimizer state |
| New checkpoints | ‚úÖ Perfect | Resume works seamlessly |
| Documentation | ‚úÖ Complete | 4 guides + 3 scripts |

## üöÄ Next Steps

1. **On Lightning.ai, run:**
   ```bash
   python smart_train_amharic.py
   ```

2. **Watch for startup message:**
   ```
   [Extended Vocab Fix] Detected extended vocabulary: 24000 tokens
   [Extended Vocab Fix] Gradient hooks registered
   ```

3. **Monitor loss (first 5k steps):**
   - Should drop below 3.0 by step 5k
   - If stuck at 4.5, report back immediately

4. **If interrupted:**
   ```bash
   # Just rerun same command:
   python smart_train_amharic.py
   ```

**That's it! The code handles everything automatically.**

---

**Questions?** See `COMPLETE_RESUME_TRAINING_GUIDE.md` for detailed explanations.
