# Tokenizer Approaches: Extension vs From-Scratch

## Quick Decision Guide

**Use Extension (24k tokens)** if:
- ‚úÖ Want cross-lingual transfer (model learns faster)
- ‚úÖ Have multilingual data (e.g., code-switching)
- ‚úÖ Fine-tuning only (not training from scratch)

**Use From-Scratch (12k tokens)** if:
- ‚úÖ Want simpler resume (no vocab mismatch issues)
- ‚úÖ Pure single-language dataset
- ‚úÖ Script very different from English/Chinese (e.g., Ethiopic, Arabic, Thai)
- ‚úÖ Don't need cross-lingual capability

## Approach 1: Extension (Current - 24,000 tokens)

### How It Works
```bash
python tools/tokenizer/extend_bpe.py \
  --base-model checkpoints/bpe.model \
  --manifests dataset/manifest.jsonl \
  --output-model tokenizers/amharic_extended_bpe.model \
  --target-size 24000
```

### Token Layout
- IDs 0-11999: Base (English/Chinese) - **FROZEN during training**
- IDs 12000-23999: Amharic (new) - **TRAINABLE**

### Pros
‚úÖ Preserves cross-lingual knowledge
‚úÖ Model can handle English/Chinese/Amharic
‚úÖ Faster convergence (transfer learning)
‚úÖ Official IndexTTS2 approach

### Cons
‚ùå Vocab mismatch breaks resume (24001 vs 24000)
‚ùå More complex (gradient hooks needed)
‚ùå Larger model size
‚ùå 3.4% of parameters frozen

### Resume Issue & Fix
Problem: If checkpoint has 24001 tokens but tokenizer has 24000:
- Optimizer state incompatible
- Training stops learning (losses stuck)

Fix: Code detects mismatch, skips optimizer load, uses fresh optimizer
See: `TRAINING_STUCK_FIX_COMPLETE.md`

---

## Approach 2: From-Scratch (Alternative - 12,000 tokens)

### How It Works
```bash
# 1. Collect corpus
python tools/collect_amharic_corpus.py \
  --manifests dataset/manifest.jsonl \
  --output amharic_corpus.txt

# 2. Train standalone tokenizer
python tools/tokenizer/train_standalone_bpe.py \
  --corpus amharic_corpus.txt \
  --output tokenizers/amharic_standalone_bpe.model \
  --vocab-size 12000 \
  --character-coverage 0.9999 \
  --user-defined-symbols "·ç¢,·ç£,·ç§,·ç•,·çß,·ç®,·ç°"
```

### Token Layout
- IDs 0-11999: Amharic only - **ALL TRAINABLE**

### Pros
‚úÖ No vocab mismatch (always 12000)
‚úÖ Resume works perfectly (no optimizer issues)
‚úÖ Simpler training (no gradient hooks)
‚úÖ 100% of embeddings trainable
‚úÖ Smaller model size

### Cons
‚ùå No cross-lingual transfer
‚ùå Can't handle English/Chinese text
‚ùå Slower initial convergence
‚ùå Not official IndexTTS2 approach

### When to Use
Perfect for:
- Pure Amharic datasets (no English mixing)
- Avoiding resume complications
- Scripts very different from English/Chinese
- Monolingual TTS applications

---

## Comparison Table

| Feature | Extension (24k) | From-Scratch (12k) |
|---------|----------------|--------------------|
| Vocab Size | 24,000 | 12,000 |
| Cross-lingual | ‚úÖ Yes | ‚ùå No |
| Resume Issues | ‚ö†Ô∏è Possible | ‚úÖ None |
| Training Speed | ‚ö° Fast (transfer) | üê¢ Slower (cold start) |
| Complexity | üîß Complex | ‚ú® Simple |
| Model Size | üì¶ Larger | üì¶ Smaller |
| Trainable % | 96.6% | 100% |
| Use Case | Multilingual | Monolingual |

---

## Your Specific Case (Amharic)

### Current Situation
- Using Extension approach (24k)
- Hit vocab mismatch bug (24001 vs 24000)
- Training stuck (losses not improving)

### Options

#### Option A: Fix Extension Approach ‚úÖ RECOMMENDED
1. Apply the vocab mismatch fix (already provided)
2. Delete corrupted checkpoints: `rm -rf trained_ckpts_fixed/`
3. Start fresh training with fix
4. Benefits: Keeps cross-lingual transfer, official approach

#### Option B: Switch to From-Scratch üîÑ ALTERNATIVE
1. Create corpus: `python tools/collect_amharic_corpus.py`
2. Train 12k tokenizer: `python tools/tokenizer/train_standalone_bpe.py`
3. Rerun preprocessing with new tokenizer
4. Train from base checkpoint (will work with 12k vocab)
5. Benefits: Simpler, no resume issues

---

## Implementation: From-Scratch Approach

If you choose to try the from-scratch approach:

### Step 1: Create Corpus
```bash
python tools/collect_amharic_corpus.py \
  --input preprocessed_amharic/train_pairs.jsonl \
  --output amharic_corpus.txt \
  --min-length 3
```

### Step 2: Train Standalone Tokenizer
```bash
python tools/tokenizer/train_standalone_bpe.py \
  --corpus amharic_corpus.txt \
  --output tokenizers/amharic_standalone_12k.model \
  --vocab-size 12000 \
  --character-coverage 0.9999 \
  --user-defined-symbols "·ç¢,·ç£,·ç§,·ç•,·çß,·ç®,·ç°"
```

### Step 3: Rerun Preprocessing
```bash
python tools/preprocess_data.py \
  --manifest dataset/manifest.jsonl \
  --output-dir preprocessed_amharic_12k \
  --tokenizer tokenizers/amharic_standalone_12k.model \
  --language am
```

### Step 4: Generate Pairs
```bash
python tools/build_gpt_prompt_pairs.py \
  --input-manifest preprocessed_amharic_12k/train_manifest.jsonl \
  --output-manifest preprocessed_amharic_12k/train_pairs.jsonl \
  --num-pairs 2
```

### Step 5: Train
```bash
python trainers/train_gpt_v2.py \
  --train-manifest preprocessed_amharic_12k/train_pairs.jsonl \
  --val-manifest preprocessed_amharic_12k/val_pairs.jsonl \
  --tokenizer tokenizers/amharic_standalone_12k.model \
  --config checkpoints/config.yaml \
  --base-checkpoint checkpoints/gpt.pth \
  --output-dir trained_ckpts_12k \
  --epochs 10 \
  --learning-rate 5e-6 \
  --amp
```

**Important:** The base checkpoint (gpt.pth) has 12k vocab, so it will work!
The model will reinitialize text embeddings for your Amharic tokens.

---

## Recommendation

**For your case:** I recommend **Option A (fix extension approach)**

Why:
1. ‚úÖ You've already done all the work (dataset, preprocessing)
2. ‚úÖ Fix is simple (code already provided)
3. ‚úÖ Keeps cross-lingual transfer benefits
4. ‚úÖ Matches official approach

But Option B is **valid** if:
- You want to avoid any future resume complications
- Don't need English/Chinese support
- Prefer simpler, more robust training

---

**Bottom Line:** Both approaches work! Extension is faster/better for multilingual, from-scratch is simpler/more robust for monolingual.
